# SignVision AR - Professional Grade Object Tracking

> **Advanced AR Edition** with Google Lens-style tracking, motion prediction, and camera compensation

Real-time road sign detection with **professional-grade AR tracking** for visually impaired users. Labels stick to objects in 3D space like Google Lens, ARCore, and ARKit.

## ğŸ¯ Key Features

### Professional AR Tracking
- âœ… **Kalman-like prediction** - Predicts object positions between detections
- âœ… **Camera motion compensation** - Uses gyroscope/accelerometer data
- âœ… **Adaptive smoothing** - More responsive for fast-moving objects
- âœ… **Multi-criteria matching** - IoU + center distance for robust tracking
- âœ… **Persistent labels** - Objects stay visible for 2 seconds without detection
- âœ… **10-frame prediction** - Continues tracking temporarily out-of-view objects

### Detection Speed
- âš¡ **6.6 FPS detection** (150ms intervals)
- âš¡ **512x384 resolution** for 8x faster processing
- âš¡ **40% image quality** for maximum speed
- âš¡ Pure **Gemini AI backend** (no local models needed)

### Visual Design
- ğŸ¨ **Google Lens style** - Rounded labels, modern UI
- ğŸ¨ **Active vs tracked** - Different styles for detected vs predicted objects
- ğŸ¨ **Glow effects** - Active detections have glowing borders
- ğŸ¨ **System fonts** - Native look and feel

## ğŸš€ Quick Start

### Requirements
- Python 3.9+
- Google Gemini API key
- Modern browser with camera access

### Local Development

1. **Clone and setup**:
```bash
cd SignVision-AR
pip install -r requirements.txt
```

2. **Configure environment**:
```bash
cp env.example .env
# Add your GEMINI_API_KEY to .env
```

3. **Run server**:
```bash
python server.py
# Or: ./run_server.sh
```

4. **Open browser**:
```
http://localhost:8000
```

### Deploy to Vercel

1. **Push to GitHub**:
```bash
git add -A
git commit -m "Initial commit - SignVision AR"
git remote add origin https://github.com/YOUR_USERNAME/signvision-ar.git
git push -u origin main
```

2. **Deploy on Vercel**:
   - Go to [vercel.com](https://vercel.com)
   - Click "Add New" â†’ "Project"
   - Import your GitHub repository
   - Add environment variable: `GEMINI_API_KEY`
   - Click "Deploy"

## ğŸ“± How AR Tracking Works

### Object Lifecycle
```
Detection â†’ Match to existing track â†’ Update position with smoothing
    â†“              â†“                           â†“
New object?   Lost object?              Still tracking?
    â†“              â†“                           â†“
Create track   Predict position        Keep label stuck!
               for 2 seconds
```

### Tracking Algorithm
1. **Predict** next positions using velocity + camera motion
2. **Match** detections using IoU + center distance
3. **Update** positions with adaptive smoothing
4. **Persist** labels for 2 seconds even without detection
5. **Remove** very stale objects (>2s or >10 missed frames)

### Camera Motion Compensation
- Uses device **gyroscope** for rotation tracking
- Uses device **accelerometer** for motion detection
- Compensates label positions based on camera movement
- Makes labels appear "stuck" to real-world objects

## ğŸ® Controls

- **Start** - Begin real-time detection
- **Pause** - Freeze current detections
- **Record** - Start dashcam recording (30s clips)
- **Settings** - Adjust voice alerts, sensitivity, etc.

## ğŸ”§ Configuration

Key parameters in `script.js`:

```javascript
processingInterval: 150,     // Detection frequency (ms)
trackingThreshold: 0.15,     // IoU matching threshold
smoothingFactor: 0.25,       // Position smoothing strength
maxTrackingAge: 2000,        // Keep labels for 2 seconds
maxMissedFrames: 10,         // Predict up to 10 frames
minConfidence: 0.25          // Minimum detection confidence
```

## ğŸ—ï¸ Architecture

### Frontend (`script.js`)
- Camera capture & AR overlay
- Professional object tracking with motion prediction
- Device sensor integration (gyro, accel)
- Real-time canvas rendering

### Backend (`server.py` / `api/index.py`)
- FastAPI server for local development
- Serverless function for Vercel deployment
- Google Gemini AI vision processing
- Detection result formatting

## ğŸ¯ Detection Types

- ğŸš¶ **Pedestrians** (Yellow)
- ğŸš— **Vehicles** (Yellow)
- ğŸ›‘ **Stop Signs** (Red)
- ğŸš¦ **Traffic Lights** (Red)
- ğŸš² **Bicycles** (Yellow)
- ğŸï¸ **Motorcycles** (Yellow)
- ğŸ• **Animals** (Yellow)

## ğŸŒ Browser Support

- âœ… Chrome/Edge (desktop & mobile)
- âœ… Safari (iOS 14+)
- âœ… Firefox (desktop)
- âš ï¸ Requires HTTPS for camera access

## ğŸ“¦ Technologies

- **Frontend**: Vanilla JavaScript, Canvas API, Web APIs
- **Backend**: Python, FastAPI, Google Gemini AI
- **Deployment**: Vercel (serverless)
- **PWA**: Service Worker, Web App Manifest

## ğŸ”’ Privacy

- All camera processing is real-time
- No video/images stored on servers
- Dashcam recordings stored locally (IndexedDB)
- Only detection metadata sent to API

## ğŸ› Troubleshooting

### Labels jumping around?
- Check device motion permissions
- Ensure smooth camera movement
- Adjust `smoothingFactor` (lower = smoother)

### Slow detection?
- Check network connection to Gemini API
- Reduce `processingInterval` (150ms recommended)
- Ensure good lighting conditions

### Objects not tracking?
- Adjust `trackingThreshold` (lower = more lenient)
- Check `maxTrackingAge` setting
- Ensure objects are clearly visible

## ğŸ“„ License

MIT License - feel free to use and modify!

## ğŸ™ Credits

Built with Google Gemini AI for real-time vision processing.

---

**SignVision AR** - Professional AR tracking for accessibility ğŸ¯âœ¨
